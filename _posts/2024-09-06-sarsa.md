---
layout: post
title: "SARSA: On-Policy TD Control"
date: 2024-09-06
tags: ["Reinforcement Learning"]
---

SARSA is an on-policy temporal difference (TD) learning algorithm

---
### Intuition

- SARSA stands for State-Action-Reward-State-Action
- It is an example of on-policy TD prediction methods for the control problem
    - control problem deals with estimating action-value functions
-  It updates Q-values based on the action performed in the current state and the action performed in the next state

---

### Update Rule

- For an on-policy method we must estimate the action value $q_{\pi}(s, a)$ for the current behavior policy $\pi$ and for all states `s` and actions `a`

$$ {q}(S_t,A_t) \leftarrow {q}(S_t,A_t) + \alpha \left (R_{t+1} + \gamma q(S_{t+1},A_{t+1}) - {q}(S_t,A_t) \right ) $$

- This update is done after every transition from a state $S_t$ unless it is a terminal state $S_T$

- If $S_{t+1}$ is terminal then $q(S_{t+1},A_{t+1}) = 0$

---

### Policy

- SARSA typically uses an $\epsilon$-greedy policy:
    - With probability $1-\epsilon$, choose the action with the highest Q-value
    - With probability $\epsilon$, choose a random action

---

### Hand Calculation

**Example**

- Let's take an example of 3*3 Gridworld 

| 1 | 2 | 3 |
| 4 | 5 | 6 |
| 7 | 8 | 9 |

- Start State: (1,1) or cell 1 (top-left corner)
- Goal State: (3,3) or cell 9 (bottom-right corner)
- Actions: Up, Down, Left, Right
- Reward Structure: +1 for reaching the goal (cell 9), and 0 for all other cells.
- Initial Q-values: Assume all Q-values are initialized to 0.
- Learning Rate ($\alpha$): 0.5
- Discount Factor ($\gamma$): 0.9