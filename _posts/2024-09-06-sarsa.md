---
layout: post
title: "SARSA: On-Policy TD Control"
date: 2024-09-06
tags: ["Reinforcement Learning"]
---

SARSA is an on-policy temporal difference (TD) learning algorithm

---
### Intuition

- SARSA stands for State-Action-Reward-State-Action
- It is an example of on-policy TD prediction methods for the control problem
    - control problem deals with estimating action-value functions
-  It updates Q-values based on the action performed in the current state and the action performed in the next state

---

### Update Rule

- For an on-policy method we must estimate the action value $q_{\pi}(s, a)$ for the current behavior policy $\pi$ and for all states `s` and actions `a`

$$ {q}(S_t,A_t) \leftarrow {q}(S_t,A_t) + \alpha \left (R_{t+1} + \gamma q(S_{t+1},A_{t+1}) - {q}(S_t,A_t) \right ) $$

- This update is done after every transition from a state $S_t$ unless it is a terminal state $S_T$

- If $S_{t+1}$ is terminal then $q(S_{t+1},A_{t+1}) = 0$

---

### Policy

- SARSA typically uses an $\epsilon$-greedy policy:
    - With probability $1-\epsilon$, choose the action with the highest Q-value
    - With probability $\epsilon$, choose a random action

---

### Hand Calculation

**Example**

Let's take an example of 3*3 Gridworld 

| 1 | 2 | 3 |
| 4 | 5 | 6 |
| 7 | 8 | 9 |

**Goal**

Agent needs to go from 1 to 9

where there are 9 states that the agent can be in