---
layout: post
title: "SARSA: On-Policy TD Control"
date: 2024-09-06
tags: ["Reinforcement Learning"]
---

SARSA is an on-policy temporal difference (TD) learning algorithm

---
### Intuition

- SARSA stands for State-Action-Reward-State-Action
- It is an example of on-policy TD prediction methods for the control problem
    - control problem deals with estimating action-value functions
-  It updates Q-values based on the action performed in the current state and the action performed in the next state

---

### Update Rule

- For an on-policy method we must estimate the action value $q_{\pi}(s, a)$ for the current behavior policy $\pi$ and for all states `s` and actions `a`

$$ {Q}(S_t,A_t) \leftarrow {Q}(S_t,A_t) + \alpha \left (R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - {Q}(S_t,A_t) \right ) $$

- This update is done after every transition from a state $S_t$ unless it is a terminal state $S_T$

- If $S_{t+1}$ is terminal then $q(S_{t+1},A_{t+1}) = 0$

---

### Policy

- SARSA typically uses an $\epsilon$-greedy policy:
    - With probability $1-\epsilon$, choose the action with the highest Q-value
    - With probability $\epsilon$, choose a random action

---

### Hand Calculation

#### Example

- Let's take an example of 2*2 Gridworld 

```
1 - 2
3 - 4
```

#### Initial Conditions
- Start State: (1,1) or cell 1 (top-left corner)
- Goal State: (2,3) or cell 4 (bottom-right corner)
- Actions: Up, Down, Left, Right
- Reward Structure: +1 for reaching the goal (cell 9), and 0 for all other cells.
- Initial Q-values: Assume all Q-values are initialized to 0.
- Learning Rate ($\alpha$): 0.5
- Discount Factor ($\gamma$): 0.9

Initial Q-table looks like this

|   | 1, Up  | 1, Down | 1, Left | 1, Right | 2, Up | 2, Down | 2, Left | 2, Right |
| - | -------- | ------- | ------- | -------- | ----- | ------- | ------- | -------- |
| 1, Up | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| 1, Down | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| 1, Left | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| 1, Right | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| 2, Up | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| 2, Down | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| 2, Left | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |
| 2, Right | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |

#### Episode 1

Let's assume the agent follows the following path during episode 1

$$ State 1 \rightarrow Right \rightarrow State 2 \rightarrow Down \rightarrow State 5 \rightarrow Right \rightarrow State 6 \rightarrow Down \rightarrow State 9 $$

**Step 1**

$$ {Q}(1,Right) \leftarrow {Q}(1,Right) + \alpha \left (R_{t+1} + \gamma Q(2,Down) - {Q}(1,Right) \right ) $$

