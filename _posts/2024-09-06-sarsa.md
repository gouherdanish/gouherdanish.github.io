---
layout: post
title: "SARSA: On-Policy TD Control"
date: 2024-09-06
tags: ["Reinforcement Learning"]
---

SARSA is an on-policy temporal difference (TD) learning algorithm

---
### Intuition

- SARSA stands for State-Action-Reward-State-Action
- It is an example of on-policy TD prediction methods for the control problem
    - control problem deals with estimating action-value functions
-  It updates Q-values based on the action performed in the current state and the action performed in the next state

---

### On-Policy Learning

- It balances exploration and exploitation using the same policy. 
- For instance, an epsilon-greedy policy might be used, where the agent usually follows the best-known action (exploitation) but occasionally explores random actions (exploration).
- The policy being evaluated and improved is the one actually used to select actions.

---

### Update Rule

- For an on-policy method we must estimate the action value $q_{\pi}(s, a)$ for the current behavior policy $\pi$ and for all states `s` and actions `a`

$$ {Q}(S_t,A_t) \leftarrow {Q}(S_t,A_t) + \alpha \left (R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - {Q}(S_t,A_t) \right ) $$

- This update is done after every transition from a state $S_t$ unless it is a terminal state $S_T$

- If $S_{t+1}$ is terminal then $q(S_{t+1},A_{t+1}) = 0$

---

### Hand Calculation

#### Example

- Let's take an example of 2*2 Gridworld 

```
1 - 2
|   |
3 - 4
```

#### Initial Conditions
- Start State: (0,0) or cell 1 (top-left corner)
- Goal State: (2,2) or cell 4 (bottom-right corner)
- Actions: Up, Down, Left, Right
- Reward Structure: +1 for reaching the goal (cell 9), and 0 for all other cells.
- Initial Q-values: Assume all Q-values are initialized to 0.
- Learning Rate ($\alpha$): 0.5
- Discount Factor ($\gamma$): 0.9

Initial Q-table looks like this

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0 | 0 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |

#### Episode 1

Let's assume the agent follows the following path during episode 1

$$ State 1 \rightarrow Right \rightarrow State 2 \rightarrow Down \rightarrow State 4 $$

**Step 1**

- Agent moves right from state 1 to state 2
- No reward in the next state, so $R_{t+1} = 0$

$$ {Q}(1,Right) \leftarrow {Q}(1,Right) + \alpha \left (R_{t+1} + \gamma Q(2,Down) - {Q}(1,Right) \right ) $$

$$ {Q}(1,Right) \leftarrow 0 + 0.5(0 + 0.9*0 - 0) $$

$$ {Q}(1,Right) \leftarrow 0 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0 | 0 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |

**Step 2**

- Agent moves down from state 2 to state 4
- Since 4 is goal state, so $R_{t+1} = 1$
- Since 4 is goal state, any action that takes agent away from 4 should have no value, so $Q(4,a') = 0$

$$ {Q}(2,Down) \leftarrow {Q}(2,Down) + \alpha \left (R_{t+1} + \gamma Q(4,a') - {Q}(2,Down) \right ) $$

$$ {Q}(2,Down) \leftarrow 0 + 0.5(1 + 0.9*0 - 0) $$

$$ {Q}(2,Down) \leftarrow 0.5 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0 | 0.5 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |

_Episode 1 ends_

#### Episode 2

Let's assume the agent follows the following path during episode 2

$$ State 1 \rightarrow Right \rightarrow State 2 \rightarrow Up \rightarrow State 2 \rightarrow Down \rightarrow State 4 $$

**Step 1**

- Agent selects action RIGHT - Exploration
- Agent moves from state 1 to state 2
- Since 2 is not the goal state, so $R_{t+1} = 0$

- Applying Update rule

$$ {Q}(1,Right) \leftarrow {Q}(1,Right) + \alpha \left (R_{t+1} + \gamma Q(2,Up) - {Q}(1,Right) \right ) $$

$$ {Q}(1,Right) \leftarrow 0 + 0.5(0 + 0.9*0 - 0) $$

$$ {Q}(1,Right) \leftarrow 0 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0 | 0.5 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |

**Step 2**

- Agent selects action UP - Exploration
- Agent stays at state 2
- Since 2 is not the goal state, so $R_{t+1} = 0$

$$ {Q}(2,Up) \leftarrow {Q}(2,Up) + \alpha \left (R_{t+1} + \gamma Q(2,Down) - {Q}(2,Up) \right ) $$

$$ {Q}(2,Up) \leftarrow 0 + 0.5(0 + 0.9*0.5 - 0) $$

$$ {Q}(2,Up) \leftarrow 0.225 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0.225 | 0.5 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |

**Step 3**

- Agent selects action DOWN - Exploitation
- Agent moves from state 2 to state 4
- Since 3 is not the goal state, so $R_{t+1} = 0$

$$ {Q}(2,Down) \leftarrow {Q}(2,Down) + \alpha \left (R_{t+1} + \gamma \max_{a'} Q(4,a') - {Q}(2,Down) \right ) $$

$$ {Q}(2,Down) \leftarrow 0.5 + 0.5(1 + 0.9*0 - 0.5) $$

$$ {Q}(2,Down) \leftarrow 0.75 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0.225 | 0.75 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |

_Episode 2 ends_


#### Episode 3

Let's assume the agent follows the following path during episode 2

$$ State 1 \rightarrow Down \rightarrow State 3 \rightarrow Up \rightarrow State 1 \rightarrow Down \rightarrow State 3 \rightarrow Right \rightarrow State 4 $$

**Step 1**

- Agent moves down from state 1 to state 3
- Since 3 is not the goal state, so $R_{t+1} = 0$

$$ {Q}(1,Down) \leftarrow {Q}(1,Down) + \alpha \left (R_{t+1} + \gamma Q(3,Up) - {Q}(1,Down) \right ) $$

$$ {Q}(1,Down) \leftarrow 0 + 0.5(0 + 0.9*0 - 0) $$

$$ {Q}(1,Down) \leftarrow 0 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0.225 | 0.75 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |


**Step 2**

- Agent moves up from state 3 to state 1
- Since 1 is not the goal state, so $R_{t+1} = 0$

$$ {Q}(3,Up) \leftarrow {Q}(3,Up) + \alpha \left (R_{t+1} + \gamma Q(1,Down) - {Q}(3,Up) \right ) $$

$$ {Q}(3,Up) \leftarrow 0 + 0.5(0 + 0.9*0 - 0) $$

$$ {Q}(3,Up) \leftarrow 0 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0.225 | 0.75 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |


**Step 3**

- Agent moves down from state 1 to state 3
- Since 3 is not the goal state, so $R_{t+1} = 0$

$$ {Q}(1,Down) \leftarrow {Q}(1,Down) + \alpha \left (R_{t+1} + \gamma Q(3,Right) - {Q}(1,Down) \right ) $$

$$ {Q}(1,Down) \leftarrow 0 + 0.5(0 + 0.9*0 - 0) $$

$$ {Q}(1,Down) \leftarrow 0 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0.225 | 0.75 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |


**Step 4**

- Agent moves down from state 3 to state 4
- Since 4 is goal state, so $R_{t+1} = 1$
- Since 4 is goal state, any action that takes agent away from 4 should have no value, so $Q(4,a') = 0$

$$ {Q}(3,Right) \leftarrow {Q}(3,Right) + \alpha \left (R_{t+1} + \gamma Q(4,a') - {Q}(3,Right) \right ) $$

$$ {Q}(3,Right) \leftarrow 0 + 0.5(1 + 0.9*0 - 0) $$

$$ {Q}(3,Right) \leftarrow 0.5 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0.225 | 0.75 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0.5 |
| 4 | 0 | 0 | 0 | 0 |


_Episode 3 ends_


### Implementation

- We provide a possible implementation of the Gridworld Bot using SARSA based updates

**Assumptions**

_Environment_
- Grid size is assumed constant as 2*2
- Start Pos - Top Left Corner i.e. (0,0)
- Goal Position - Bottom right corner i.e. (1,1)
- Step size - 1

_Agent_
- Learning Rate ($\alpha$) - 0.5
- Discount Factor ($\gamma$) - 0.9
- Greed Factor ($\epsilon$) - 0.1

**Code**
```
from entities.env import GridWorld
from constants import AgentParams, EnvParams
from factory.game_factory import GameFactory
from factory.agent_factory import AgentFactory

# Inputs
>>> method = 'sarsa'
>>> episodes = 2

# GridWorld Environment 
# - EnvParams contains grid dimension, start position, goal position
>>> env = GridWorld(env_params=EnvParams())

# Qlearning Agent
# - AgentParams contains epsilon(greedy factor), gamma(discount factor), alpha(learning rate)
>>> agent = AgentFactory.get(type=method, states=env.states, agent_params=AgentParams())

# Agent-Environment Interaction
# - Trains the agent over number of episodes and agent builds estimates of each state and action
# - and learns to navigate the grid to reah goal position
>>> game = GameFactory.get(method=method,env=env,agent=agent)
>>> game.run(episodes=episodes)

>>> print([(s,a,q) for (s,a), q in self.agent.estimates.items() if q.value != 0])
[(State(0,0), <Actions.RIGHT: 'right'>, Q(0.225)), (State(0,1), <Actions.DOWN: 'down'>, Q(0.75))]

```

---
### Conclusion
- SARSA is a fundamental reinforcement learning technique
- We implemented a SARSA Bot that learns to navigate a Gridworld environment
