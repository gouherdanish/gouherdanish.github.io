---
layout: post
title: "Understanding Softmax Function"
date: 2024-10-28
tags: ["Machine Learning"]
---


---
### Definition
- To interpret these logits as probabilities, we usually apply the softmax function, which converts the logits into values between 0 and 1 that sum to 1
- Softmax is defined as follows

$$ Softmax(z) = \frac {e^{z_j}}{\sum_{j} {1+e^{z_j}}} $$

---
### Implementation
```
def softmax(z):
    return z.exp()/(z.exp().sum(-1)).unsqueeze(1)

>>> yhat = softmax(z)
>>> yhat
tensor([[0.2312, 0.1402, 0.6285],
        [0.0674, 0.8214, 0.1112]])
```

---
### Validation
- Note that, for each example row, the probabilities now sum to 1

```
0.2312 + 0.1402 + 0.6285 = 1.0
0.0674 + 0.8214 + 0.1112 = 1.0
```

---
### Verification
- This can be verified using PyTorch implementation

```
>>> import torch.nn as nn
>>> yhat = nn.functional.softmax(z)
>>> yhat
tensor([[0.2312, 0.1402, 0.6285],
        [0.0674, 0.8214, 0.1112]])
```

---
### Limitations

- Calculating softmax function involves exponentiation of logits
- When logits are large (positive or negative), exponentiation can lead to very large or very small probabilities, approaching the limits of floating-point precision
- This can produce `nan` values as shown below

```
>>> z = torch.tensor([[-1000.0, -1000.0, 1000.0]])
>>> yhat = softmax(z)
>>> yhat
tensor([[0.0000, 0.0000,    nan]])
```