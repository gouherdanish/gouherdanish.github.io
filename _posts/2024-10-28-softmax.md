---
layout: post
title: "Understanding Softmax Function"
date: 2024-10-28
tags: ["Machine Learning"]
---


---
## Definition
- Raw outputs from model are called Logits
- To interpret these logits as probabilities, we usually apply the softmax function, which converts the logits into values between 0 and 1 that sum to 1
- Mathematically, Softmax is defined as follows

$$ Softmax(z) = \frac {e^{z_i}}{\sum_{j=1}^C e^{z_j}} = p_i $$

- These represent the predicted probability distribution $p_i$

- Later, we use Cross Entropy Loss to quantify how good the predicted distribution is compared to the true distribution

$$ L = - \sum_{i=1}^n y_i \log p_i $$

---
## Implementation

### Formulation

- Let's say, input data `X` is converted to logits `z` after passing it through `model`

```
>>> z = model(X)    # X: input data, z: logits
>>> z
tensor([[1,0.5,2],
        [0.5,3,1]])
```

- Using Tensor algebra, softmax can be implemented in code as follows

```
def softmax(z):
    return z.exp()/(z.exp().sum(-1)).unsqueeze(1)

>>> yhat = softmax(z)
>>> yhat
tensor([[0.2312, 0.1402, 0.6285],
        [0.0674, 0.8214, 0.1112]])
```

---
### Validation
- To validate that the function works, we can notice that the probabilities sum to 1 for each example row

```
0.2312 + 0.1402 + 0.6285 = 1.0
0.0674 + 0.8214 + 0.1112 = 1.0
```

---
### Verification
- This can be verified against PyTorch

```
>>> import torch.nn as nn
>>> yhat = nn.functional.softmax(z)
>>> yhat
tensor([[0.2312, 0.1402, 0.6285],
        [0.0674, 0.8214, 0.1112]])
```

---
### Limitations

**Case 1 - Very Large Logits**
- Calculating softmax function involves exponentiation of logits
- When logits are large (positive or negative), exponentiation can lead to very large or very small probabilities, approaching the limits of floating-point precision
- This can produce `nan` values as shown below

```
>>> z = torch.tensor([[-1000.0, -1000.0, 1000.0]])
>>> yhat = softmax(z)
>>> yhat
tensor([[0., 0.,    nan]])
```

- However the PyTorch version is able to handle this case
```
>>> nn.functional.softmax(z)
tensor([[0., 0., 1.]])
```

**Case 2 - Highly Negative Logits**

- If logits are highly negative, Taking exponentiation will make them 0 which will cause NaN issue due to zero division

```
>>> z = torch.tensor([[-1000.0, -2000.0, -500.0]])
>>> yhat = softmax(z)
>>> yhat, nn.functional.softmax(z)
tensor([[nan, nan, nan]])
```

- However the PyTorch version is able to handle this case also
```
>>> nn.functional.softmax(z)
tensor([[0., 0., 1.]])
```

- Due to the above limitations, using user-defined softmax may lead to numerical instability issues.

### Normalized Softmax Function

- We know Softmax function is defined as follows

$$ Softmax(z) = \frac {e^{z_j}}{\sum_{j=1}^C e^{z_j}} $$

- We can write it as follows

$$ Softmax(z) = \frac {e^{z_j}}{\sum_{j=1}^C e^{z_j}} \frac {e^{-z_{max}}}{e^{-z_{max}}} $$

$$ Softmax(z) = \frac {e^{z_j-z_{max}}}{\sum_{j=1}^C e^{z_j-z_{max}}}$$

- Using Tensor algebra, we can implement this in PyTorch as follows

```
def softmax_norm(z):
    z_max, _ = torch.max(z,-1,keepdim=True)
    z = z - z_max
    return z.exp()/(z.exp().sum(-1)).unsqueeze(1)
```

- This modified implementation can handle both the above limitations

```
# Large Logits
>>> z = torch.tensor([[-1000.0, -1000.0, 1000.0]])
>>> softmax_norm(z), nn.functional.softmax(z)
(tensor([[0., 0., 1.]]), tensor([[0., 0., 1.]]))

# Highly Negative Logits
>>> z = torch.tensor([[-1000.0, -2000.0, -500.0]])
>>> yhat = softmax(z)
>>> softmax_norm(z), nn.functional.softmax(z)
(tensor([[0., 0., 1.]]), tensor([[0., 0., 1.]]))
```

### Log Softmax Function

- Our goal is to calculate Cross Entropy Loss which is defined as follows

$$ L = - \sum_{i=1}^n y_i \log p_i $$

- Instead of calculating softmax first and then taking log, we can calculate log of softmax in one step

- Our normalized softmax implementation looks like this

$$ p_i = \frac {e^{z_i-z_{max}}}{\sum_{j=1}^C e^{z_j-z_{max}}}$$

- Taking `log` both sides, we get

$$ \log p_i = \log \frac {e^{z_i-z_{max}}}{\sum_{j=1}^C e^{z_j-z_{max}}} $$

$$ \log p_i = \log {e^{z_i-z_{max}}} - \log {\sum_{j=1}^C e^{z_j-z_{max}}} $$

$$ \log p_i = z_i-z_{max} - \log {\sum_{j=1}^C e^{z_j-z_{max}}} $$

- Using Tensor algebra, we can implement this in PyTorch as follows

```
def log_softmax(z):
    z_max, _ = torch.max(z,-1,keepdim=True)
    return z - z_max - (z-z_max).exp().sum(-1).log().unsqueeze(1)
```

- This implementation is also able to handle both the above limitations

```
# Large Logits
>>> z = torch.tensor([[-1000.0, -1000.0, 1000.0]])
>>> log_softmax_norm(z), nn.functional.log_softmax(z)
(tensor([[-2000., -2000.,     0.]]), tensor([[-2000., -2000.,     0.]]))

# Highly Negative Logits
>>> z = torch.tensor([[-1000.0, -2000.0, -500.0]])
>>> log_softmax_norm(z), nn.functional.log_softmax(z)
(tensor([[ -500., -1500.,     0.]]), tensor([[ -500., -1500.,     0.]]))
```

Note: 
- Our output matches perfectly with PyTorch function output

---

## Conclusion

- We explored basic Softmax function, its limitations and provided modified implementations to handle these limitations
- We also compared our output with PyTorch function output and saw they match perfectly