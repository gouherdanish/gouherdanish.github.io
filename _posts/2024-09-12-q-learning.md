---
layout: post
title: "Q-Learning: Off-Policy TD Control"
date: 2024-09-12
tags: ["Reinforcement Learning"]
---

Q-Learning is an off-policy temporal difference (TD) learning algorithm

---
### Intuition

- Q stands for Quality
- It is an example of off-policy TD prediction methods for the control problem
    - control problem deals with estimating action-value functions
-  It updates Q-values based on the reward received and the maximum estimated Q-value of the next state regardless of the action that the current policy would take

---

### Off-policy Learning

- It separates exploration from exploitation. 
- The agent can explore using one policy (like a random policy) while learning the optimal policy independently. 
- The learning process is not affected by the exploration strategy.
- This flexibility allows the agent to learn from different sources of experience, including past data, simulated experiences, or experiences generated by a different agent.

---

### Update Rule

- For an on-policy method we must estimate the action value $q_{\pi}(s, a)$ for the current behavior policy $\pi$ and for all states `s` and actions `a`

$$ {Q}(S_t,A_t) \leftarrow {Q}(S_t,A_t) + \alpha \left (R_{t+1} + \gamma max_{a'}Q(S_{t+1},a') - {Q}(S_t,A_t) \right ) $$

- This update is done after every transition from a state $S_t$ unless it is a terminal state $S_T$

- If $S_{t+1}$ is terminal then $Q(S_{t+1},A_{t+1}) = 0$

---

### Hand Calculation

#### Example

- Let's take an example of 2*2 Gridworld 

```
1 - 2
|   |
3 - 4
```

#### Initial Conditions
- Start State: (1,1) or cell 1 (top-left corner)
- Goal State: (2,3) or cell 4 (bottom-right corner)
- Actions: Up, Down, Left, Right
- Reward Structure: +1 for reaching the goal (cell 9), and 0 for all other cells.
- Initial Q-values: Assume all Q-values are initialized to 0.
- Learning Rate ($\alpha$): 0.5
- Discount Factor ($\gamma$): 0.9

Initial Q-table looks like this

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0 | 0 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |

#### Episode 1

Let's assume the agent follows the following path during episode 1

$$ State 1 \rightarrow Right \rightarrow State 2 \rightarrow Down \rightarrow State 4 $$

**Step 1**

- Agent select action Right - Exploration
- Agent moves from state 1 to state 2
- No reward in the next state, so $R_{t+1} = 0$
- At 2, max q-value out of all the actions is 

$$ \max_{a'} Q(2,a') = max(Q(2,Up), Q(2,Down), Q(2,Left),Q(2,Right)) = max(0,0,0,0) = 0 $$

- Applying Update rule

$$ {Q}(1,Right) \leftarrow {Q}(1,Right) + \alpha \left (R_{t+1} + \gamma \max_{a'} Q(2,a') - {Q}(1,Right) \right ) $$

$$ {Q}(1,Right) \leftarrow 0 + 0.5(0 + 0.9*0 - 0) $$

$$ {Q}(1,Right) \leftarrow 0 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0 | 0 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |

**Step 2**

- Agent select action Down - Exploration
- Agent moves from state 2 to state 4
- Since 4 is goal state, so $R_{t+1} = 1$
- Since 4 is goal state, any action that takes agent away from 4 should have no value, so $Q(4,a') = 0$

$$ {Q}(2,Down) \leftarrow {Q}(2,Down) + \alpha \left (R_{t+1} + \gamma \max_{a'} Q(2,a') - {Q}(2,Down) \right ) $$

$$ {Q}(2,Down) \leftarrow 0 + 0.5(1 + 0.9*0 - 0) $$

$$ {Q}(2,Down) \leftarrow 0.5 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0 |
| 2 | 0 | 0.5 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |

_Episode 1 ends_

#### Episode 2

Let's assume the agent follows the following path during episode 2

$$ State 1 \rightarrow Right \rightarrow State 2 \rightarrow Up \rightarrow State 2 \rightarrow Down \rightarrow State 4 $$

**Step 1**

- Agent selects action RIGHT - Exploration
- Agent moves from state 1 to state 2
- Since 2 is not the goal state, so $R_{t+1} = 0$
- At 2, max q-value out of all the actions is 

$$ \max_{a'} Q(2,a') = max(Q(2,Up), Q(2,Down), Q(2,Left),Q(2,Right)) = max(0,0.5,0,0) = 0.5 $$

- Applying Update rule

$$ {Q}(1,Right) \leftarrow {Q}(1,Right) + \alpha \left (R_{t+1} + \gamma \max_{a'} Q(2,a') - {Q}(1,Right) \right ) $$

$$ {Q}(1,Right) \leftarrow 0 + 0.5(0 + 0.9*0.5 - 0) $$

$$ {Q}(1,Right) \leftarrow 0.225 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0.225 |
| 2 | 0 | 0.5 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |

**Step 2**

- Agent selects action UP - Exploration
- Agent stays at state 2
- Since 2 is not the goal state, so $R_{t+1} = 0$

$$ {Q}(2,Up) \leftarrow {Q}(2,Up) + \alpha \left (R_{t+1} + \gamma \max_{a'} Q(2,a') - {Q}(2,Up) \right ) $$

$$ {Q}(2,Up) \leftarrow 0 + 0.5(0 + 0.9*0.5 - 0) $$

$$ {Q}(2,Up) \leftarrow 0.225 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0.225 |
| 2 | 0.225 | 0.5 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |

**Step 3**

- Agent selects action DOWN - Exploitation
- Agent moves from state 2 to state 4
- Since 3 is not the goal state, so $R_{t+1} = 0$

$$ {Q}(2,Down) \leftarrow {Q}(2,Down) + \alpha \left (R_{t+1} + \gamma \max_{a'} Q(4,a') - {Q}(2,Down) \right ) $$

$$ {Q}(2,Down) \leftarrow 0.5 + 0.5(1 + 0.9*0 - 0.5) $$

$$ {Q}(2,Down) \leftarrow 0.75 $$

**Updated Q-table**

| State\Action | Up | Down | Left | Right |
| - | - | - | - | - |
| 1 | 0 | 0 | 0 | 0.225 |
| 2 | 0.225 | 0.75 | 0 | 0 |
| 3 | 0 | 0 | 0 | 0 |
| 4 | 0 | 0 | 0 | 0 |

_Episode 2 ends_

### Implementation

- We provide a possible implementation of the Gridworld Bot using Q-Learning based updates

**Assumptions**

_Environment_
- Grid size is assumed constant as 2*2
- Start Pos - Top Left Corner i.e. (0,0)
- Goal Position - Bottom right corner i.e. (1,1)
- Step size - 1

_Agent_
- Learning Rate ($\alpha$) - 0.5
- Discount Factor ($\gamma$) - 0.9
- Greed Factor ($\epsilon$) - 0.1

**Code**
```
from entities.env import GridWorld
from constants import AgentParams, EnvParams
from factory.game_factory import GameFactory
from factory.agent_factory import AgentFactory

# Inputs
>>> method = 'qlearning'
>>> episodes = 2

# GridWorld Environment 
# - EnvParams contains grid dimension, start position, goal position
>>> env = GridWorld(env_params=EnvParams())

# Qlearning Agent
# - AgentParams contains epsilon(greedy factor), gamma(discount factor), alpha(learning rate)
>>> agent = AgentFactory.get(type=method, states=env.states, agent_params=AgentParams())

# Agent-Environment Interaction
# - Trains the agent over number of episodes and agent builds estimates of each state and action
# - and learns to navigate the grid to reah goal position
>>> game = GameFactory.get(method=method,env=env,agent=agent)
>>> game.run(episodes=episodes)

>>> print([(s,a,q) for (s,a), q in self.agent.estimates.items() if q.value != 0])
[(State(0,0), <Actions.RIGHT: 'right'>, Q(0.225)), (State(0,1), <Actions.DOWN: 'down'>, Q(0.75))]

```

---
### Conclusion
- Q-Learning is a fundamental reinforcement learning technique
- We implemented a Q-Learning Bot that learns to navigate a Gridworld environment
