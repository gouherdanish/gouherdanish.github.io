## Entropy : Intuition, and Uses

Entropy is a very important concept in Information theory and is widely used in Machine Learning. Below we try to learn important points related to this in detail

---

### Intuition
- Entropy is a concept from information theory introduced by Claude Shannon in 1948. 
- It measures the uncertainty or randomness in a probability distribution. 
- In the context of machine learning and data science, entropy quantifies the unpredictability or impurity in a dataset.

### Definition
- Given discrete random variable X with possible outcomes $x_1, x_2, ..., x_n$ and a probability distribution $p(x)$, 
- Entropy $H(X)$ is defined as the expected value of the information content across all possible outcomes of the random variable X.

$$ H(X) = E(I(X)) $$

$$ H(X) = E(-log(p(X))) $$

$$ H(X) = - p(x_1) log(p(x_1)) - p(x_2) log(p(x_2)) ... - p(x_n) log(p(x_n)) $$

$$ H\left( X \right) =  -\sum_{i=1}^n p\left( x_i \right) log p\left( x_i \right) $$

### Properties
- **Non-Negativity** - Entropy is always non-negative
- **Maximization for Uniform Distribution** - Entropy is maximized when the distribution is uniform, meaning all outcomes are equally likely, reflecting maximum uncertainty.
- **Additivity** - Independent sources of uncertainty (from independent events) add up