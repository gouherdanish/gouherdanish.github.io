## Entropy - Measuring randomness in a probability distribution

Entropy is a very important concept in Information theory and is widely used in Machine Learning. Below we try to learn important points related to this in detail

---

### Intuition
- Entropy is a concept from information theory introduced by Claude Shannon in 1948. 
- It measures the uncertainty or randomness in a probability distribution. 
- In the context of machine learning and data science, entropy quantifies the unpredictability or impurity in a dataset.

### Definition
- Given discrete random variable X with possible outcomes $x_1, x_2, ..., x_n$ and a probability distribution $p(x)$, 
- Entropy $H(X)$ is defined as the expected value of the information content across all possible outcomes of the random variable X.

$$ H(X) = E(I(X)) $$

$$ H(X) = E(-log(p(X))) $$

$$ H(X) = - p(x_1) log(p(x_1)) - p(x_2) log(p(x_2)) ... - p(x_n) log(p(x_n)) $$

$$ H\left( X \right) =  -\sum_{i=1}^n p\left( x_i \right) log p\left( x_i \right) $$

### Properties
- **Non-Negativity** - Entropy is always non-negative
- **Maximization for Uniform Distribution** - Entropy is maximized when the distribution is uniform, meaning all outcomes are equally likely, reflecting maximum uncertainty.
- **Additivity** - Independent sources of uncertainty (from independent events) add up

### Hand Calculation
Consider an experiment where a fair coin is flipped 3 times. Here are following outcomes

$$ Sample Space = \{HHH,HHT,HTH,THH,HTT,THT,TTH,TTT\} $$

Let $X$ denote a random variable which represents the number of heads in 3 consecutive coin flips

$$ X = \{0,1,2,3\} $$

The probability $p(X)$ associated with each $X$

$$ p(X=0) = p(TTT) = 1/8 $$

$$ p(X=1) = p(HTT or THT or TTH) = 3/8 $$

$$ p(X=2) = p(HHT or HTH or THH) = 3/8 $$

$$ p(X=3) = p(HHH) = 1/8 $$

Entropy $S(X)$ can be calculated as follows,

$$ H(X) = -\sum_{i=1}^n p(x_i) log(p(x_i))$$

$$ H(X) = - {(1 \over 8)}{(log(1 \over 8))} - {(3 \over 8)}{(log(3 \over 8))} - {(3 \over 8)}{(log(3 \over 8))} - {(1 \over 8)}{(log(1 \over 8))} $$

$$ H(X) = - {{(2.07944154 + 3*0.98082925 + 3*0.98082925 + 2.07944154)} \over 8} $$

$$ H(X) = 1.2554823251787535 $$

### Formulation
```
import numpy as np

def entropy(px: numpy.ndarray):
    return -np.dot(px,np.log(px))

px = np.array([0.125,0.375,0.375,0.125])    # Represents a probability distribution
entropy(px)                                 # 1.2554823251787535
```

### OOPS Implementation
```
from random_event import RandomEvent
from random_variable import RandomVariable

events = [
    RandomEvent(x=0,prob=0.125),
    RandomEvent(x=1,prob=0.375),
    RandomEvent(x=2,prob=0.375),
    RandomEvent(x=3,prob=0.125),
]
X = RandomVariable(events)
print(X.entropy())  # 1.2554823251787535
```
For full implementation, refer following repository

Github - [Stats Concepts](https://github.com/gouherdanish/stats_concepts/blob/main/random_variable.py)